# Backpropagation continued

This section explores a few of the problems (and solutions) to neural networks and backpropagation. 

## This is part 3 of a series of github repos on neural networks

- [part 1 - simplest network](https://github.com/gokadin/ai-simplest-network)
- [part 2 - backpropagation](https://github.com/gokadin/ai-backpropagation)
- part 3 - backpropagation-continued (**you are here**)
- [part 4 - hopfield networks](https://github.com/gokadin/ai-hopfield-networks)

## Table of Contents

- [Theory](#theory)
- [Code example](#code-example)
- [References](#references)

## Theory

ðŸš§UNDER CONSTRUCTIONðŸš§

subjects to be covered:

- momentum
- over fitting/generalization/early stopping point
- vanishing/exploding gradients
- local and global minima
- batch gradient descent
- stochastic gradient descent

## Code example

ðŸš§UNDER CONSTRUCTIONðŸš§

## References

- Artificial intelligence engines by James V Stone (2019)
- Complete guide on deep learning: http://neuralnetworksanddeeplearning.com/chap2.html